{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_tree(filename):\n",
    "    docx_file = filename + '.docx'\n",
    "\n",
    "    document = Document(docx_file)\n",
    "    id_ = 0\n",
    "    text = {}\n",
    "    adj_list = {}\n",
    "    parent = {}\n",
    "    prev_titles = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.text:\n",
    "            id_ += 1\n",
    "            # get size\n",
    "            size = 0\n",
    "            if para.style.font.size != None:\n",
    "                size = para.style.font.size/12700\n",
    "            for run in para.runs:\n",
    "                if run.font.size:\n",
    "                    size = max(size, run.font.size/12700)\n",
    "            # get prev title list\n",
    "            while len(prev_titles) and prev_titles[-1]['size'] <= size:\n",
    "                prev_titles.pop()\n",
    "            if prev_titles:\n",
    "                adj_list[prev_titles[-1]['id']].append(id_)\n",
    "                parent[id_] = prev_titles[-1]['id']\n",
    "            else:\n",
    "                parent[id_] = None\n",
    "            prev_titles.append({'size': size, 'id': id_})\n",
    "            adj_list[id_] = []\n",
    "            text[id_] = preprocess_paragraph(para.text)\n",
    "    return text, adj_list, parent\n",
    "\n",
    "def create_tree(document):\n",
    "    id_ = 0\n",
    "    text = {}\n",
    "    adj_list = {}\n",
    "    parent = {}\n",
    "    prev_titles = []\n",
    "    for element in document['content']:\n",
    "        id_ += 1\n",
    "        # get size\n",
    "        size = element[1]\n",
    "        # get prev title list\n",
    "        while len(prev_titles) and prev_titles[-1]['size'] >= size:\n",
    "            prev_titles.pop()\n",
    "        if prev_titles:\n",
    "            adj_list[prev_titles[-1]['id']].append(id_)\n",
    "            parent[id_] = prev_titles[-1]['id']\n",
    "        else:\n",
    "            parent[id_] = None\n",
    "        prev_titles.append({'size': size, 'id': id_})\n",
    "        adj_list[id_] = []\n",
    "        text[id_] = preprocess_paragraph(element[0])\n",
    "    return text, adj_list, parent\n",
    "\n",
    "def create_kg(documents):\n",
    "    subject_edges = set([])\n",
    "    modifier_edges = set([])\n",
    "    subject_modifier_edges = set([])\n",
    "    relation_edges = []\n",
    "    other_edges = set([])\n",
    "\n",
    "    did = 0\n",
    "    tid = 10000\n",
    "    pid = 20000\n",
    "    sid = 30000\n",
    "    eid = 40000\n",
    "    xid = 50000\n",
    "    count = 0\n",
    "    eid_to_text = {}\n",
    "    text_to_eid = {}\n",
    "    sid_to_text = {}\n",
    "    text_to_sid = {}\n",
    "    pid_to_text = {}\n",
    "    text_to_pid = {}\n",
    "    tid_to_text = {}\n",
    "    text_to_tid = {}\n",
    "    did_to_text = {}\n",
    "    text_to_did = {}\n",
    "    xid_to_text = {}\n",
    "    text_to_xid = {}\n",
    "    # for each document get the paragraphs and topics\n",
    "    for doc in tqdm(documents):\n",
    "        if (doc['format'] != 'html' or not 'body' in doc) and doc['format'] != 'docx':\n",
    "            continue\n",
    "        if doc['name'] in text_to_did:\n",
    "            continue\n",
    "        text_to_did[doc['name']] = did\n",
    "        did_to_text[did] = doc\n",
    "        did += 1\n",
    "        # read document in python\n",
    "        if doc['format'] == 'html':\n",
    "            text, adj_list, parent = create_tree(doc)\n",
    "        else:\n",
    "            text, adj_list, parent = create_doc_tree(doc['file'])\n",
    "        # traverse doc tree to identify paragraphs and topics in the document\n",
    "        for node in adj_list:\n",
    "            if adj_list[node]:\n",
    "                if text[node] not in text_to_tid:\n",
    "                    text_to_tid[text[node]] = tid\n",
    "                    tid_to_text[tid] = text[node]\n",
    "                    tid += 1\n",
    "                if parent[node]:\n",
    "                    if text[parent[node]] not in text_to_tid:\n",
    "                        text_to_tid[text[parent[node]]] = tid\n",
    "                        tid_to_text[tid] = text[parent[node]]\n",
    "                        tid += 1\n",
    "                    other_edges.add((text_to_tid[text[node]], 'about_concept', text_to_tid[text[parent[node]]]))\n",
    "            else:\n",
    "                if text[node] not in text_to_pid:\n",
    "                    text_to_pid[text[node]] = pid\n",
    "                    pid_to_text[pid] = text[node]\n",
    "                    pid += 1\n",
    "                if parent[node]:\n",
    "                    if text[parent[node]] not in text_to_tid:\n",
    "                        text_to_tid[text[parent[node]]] = tid\n",
    "                        tid_to_text[tid] = text[parent[node]]\n",
    "                        tid += 1\n",
    "                    other_edges.add((text_to_pid[text[node]], 'about_concept', text_to_tid[text[parent[node]]]))\n",
    "                    other_edges.add((text_to_pid[text[node]], 'from_document', text_to_did[doc['name']]))\n",
    "    # convert topics to lower case\n",
    "    for tid in tqdm(tid_to_text):\n",
    "        tid_to_text[tid] = {\n",
    "            'text': tid_to_text[tid].lower(),\n",
    "            'keywords': [k[0] for k in find_keywords(tid_to_text[tid].lower())],\n",
    "            'tags': list(set([tag for k in find_keywords(tid_to_text[tid].lower()) for tag in k[1]]))\n",
    "        }\n",
    "    \n",
    "    # for each paragraph get sentences\n",
    "    for pid in tqdm(pid_to_text):\n",
    "        sentences = split_into_sentences(pid_to_text[pid])\n",
    "        for sentence in sentences:\n",
    "            sentence = {\n",
    "                'text': sentence, \n",
    "                'stemmed_tokens': get_stemmed_sentence_tokens(sentence)\n",
    "            }\n",
    "            if sentence['text'] not in text_to_sid:\n",
    "                text_to_sid[sentence['text']] = sid\n",
    "                sid_to_text[sid] = sentence\n",
    "                sid += 1\n",
    "            other_edges.add((pid, 'contains_sentence', text_to_sid[sentence['text']]))\n",
    "    \n",
    "    # for each sentence get extractions\n",
    "    for sid in tqdm(sid_to_text):\n",
    "        extractions = extract(sid_to_text[sid]['text'])\n",
    "        if extractions:\n",
    "            count += 1\n",
    "            for extraction in extractions:\n",
    "                eid_to_text[eid] = extraction\n",
    "                other_edges.add((sid, 'contains_extraction', eid))\n",
    "                eid += 1\n",
    "        else:\n",
    "            keywords = find_keywords(sid_to_text[sid]['text'])\n",
    "            for keyword in keywords:\n",
    "                if keyword[0] not in text_to_xid:\n",
    "                    text_to_xid[keyword[0]] = xid\n",
    "                    xid_to_text[xid] = keyword\n",
    "                    xid += 1\n",
    "                other_edges.add((sid, 'about_entity', text_to_xid[keyword[0]]))\n",
    "    \n",
    "    # canonicalise the obtained extractions\n",
    "    canonicalise(eid_to_text)\n",
    "    \n",
    "    # for each extraction create entities and relations\n",
    "    for eid in tqdm(eid_to_text):\n",
    "        ext = eid_to_text[eid]\n",
    "        \n",
    "        if ext['subject'][0] not in text_to_xid:\n",
    "            text_to_xid[ext['subject'][0]] = xid\n",
    "            xid_to_text[xid] = ext['subject']\n",
    "            xid += 1\n",
    "        \n",
    "        subject_edges.add((eid, 'subject', text_to_xid[ext['subject'][0]]))\n",
    "\n",
    "        if ext['object'][0] not in text_to_xid:\n",
    "            text_to_xid[ext['object'][0]] = xid\n",
    "            xid_to_text[xid] = ext['object']\n",
    "            xid += 1\n",
    "\n",
    "        relation_edges.append((eid, ext['relation'], text_to_xid[ext['object'][0]], list(ext['rel_synsets'])))\n",
    "        \n",
    "        for modifier in ext['modifiers']:\n",
    "            if modifier['m_obj'][0] not in text_to_xid:\n",
    "                text_to_xid[modifier['m_obj'][0]] = xid\n",
    "                xid_to_text[xid] = modifier['m_obj']\n",
    "                xid += 1\n",
    "\n",
    "            modifier_edges.add((eid, modifier['m_rel'], text_to_xid[modifier['m_obj'][0]]))\n",
    "            \n",
    "        for subject_modifier in ext['subject_modifiers']:\n",
    "            if subject_modifier['m_obj'][0] not in text_to_xid:\n",
    "                text_to_xid[subject_modifier['m_obj'][0]] = xid\n",
    "                xid_to_text[xid] = subject_modifier['m_obj']\n",
    "                xid += 1\n",
    "\n",
    "            subject_modifier_edges.add((eid, subject_modifier['m_rel'], text_to_xid[subject_modifier['m_obj'][0]]))\n",
    "    \n",
    "    print(did, tid, pid, sid, eid, xid, count, round(10000 * count/sid)/100)\n",
    "    \n",
    "    offset = {\n",
    "        'documents': 0,\n",
    "        'topics': len(did_to_text),\n",
    "        'paragraphs': len(did_to_text) + len(tid_to_text),\n",
    "        'sentences': len(did_to_text) + len(tid_to_text) + len(pid_to_text),\n",
    "        'extractions': len(did_to_text) + len(tid_to_text) + len(pid_to_text) + len(sid_to_text),\n",
    "        'entities':  len(did_to_text) + len(tid_to_text) + len(pid_to_text) + len(sid_to_text) + len(eid_to_text)\n",
    "    }\n",
    "    \n",
    "    vertices = {\n",
    "        'documents': list({'id': k, 'text': did_to_text[k]['name'], 'source': did_to_text[k]['link']} for k in did_to_text.keys()),\n",
    "        'topics': list({'id': k, 'text': tid_to_text[k]['text'], 'keywords': tid_to_text[k]['keywords'], 'tags': tid_to_text[k]['tags']} for k in tid_to_text.keys()),\n",
    "        'paragraphs': list({'id': k, 'text': pid_to_text[k]} for k in pid_to_text.keys()),\n",
    "        'sentences': list({'id': k, 'text': sid_to_text[k]['text'], 'stemmed_tokens': sid_to_text[k]['stemmed_tokens']} for k in sid_to_text.keys()),\n",
    "        'extractions': list({'id': k, 'body': eid_to_text[k]} for k in eid_to_text.keys()),\n",
    "        'entities': list({'id': k, 'text': xid_to_text[k][0], 'tags': xid_to_text[k][1], 'tokens': xid_to_text[k][2]} for k in xid_to_text.keys())\n",
    "    }\n",
    "    \n",
    "    triples = {\n",
    "        'main': list(other_edges),\n",
    "        'subjects': list(subject_edges),\n",
    "        'modifiers': list(modifier_edges),\n",
    "        'subject_modifiers': list(subject_modifier_edges),\n",
    "        'relations': list(relation_edges)\n",
    "    }\n",
    "    return vertices, triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.52it/s]\n",
      "100%|██████████| 126/126 [00:09<00:00, 12.63it/s]\n",
      "100%|██████████| 295/295 [00:10<00:00, 28.49it/s]\n",
      "100%|██████████| 562/562 [00:44<00:00, 12.58it/s]\n",
      "ERROR: wish\n",
      "ERROR: maintain\n",
      "ERROR: describe\n",
      "ERROR: take\n",
      "ERROR: read\n",
      "ERROR: understand\n",
      "ERROR: ask\n",
      "ERROR: say\n",
      "ERROR: say\n",
      "ERROR: use\n",
      "ERROR: use\n",
      "ERROR: allow\n",
      "ERROR: verbatim\n",
      "ERROR: submit\n",
      "ERROR: assume\n",
      "ERROR: expect\n",
      "ERROR: use\n",
      "ERROR: require\n",
      "ERROR: involve\n",
      "ERROR: should\n",
      "ERROR: clarify\n",
      "ERROR: carry\n",
      "ERROR: award\n",
      "ERROR: award\n",
      "ERROR: carry\n",
      "ERROR: use\n",
      "ERROR: award\n",
      "ERROR: convert\n",
      "ERROR: mean\n",
      "ERROR: replace\n",
      "ERROR: reflect\n",
      "ERROR: reflect\n",
      "ERROR: count\n",
      "ERROR: indicate\n",
      "ERROR: send\n",
      "ERROR: fill\n",
      "ERROR: fill\n",
      "ERROR: date\n",
      "ERROR: provide\n",
      "ERROR: allot\n",
      "ERROR: participate\n",
      "ERROR: confirm\n",
      "ERROR: confirm\n",
      "ERROR: should\n",
      "ERROR: do\n",
      "ERROR: should\n",
      "ERROR: do\n",
      "ERROR: maintain\n",
      "ERROR: provide\n",
      "ERROR: convert\n",
      "ERROR: convert\n",
      "ERROR: handle\n",
      "ERROR: handle\n",
      "ERROR: handle\n",
      "ERROR: create\n",
      "ERROR: create\n",
      "ERROR: endanger\n",
      "ERROR: endanger\n",
      "ERROR: commit\n",
      "ERROR: use\n",
      "ERROR: provide\n",
      "ERROR: use\n",
      "ERROR: revert\n",
      "ERROR: allot\n",
      "ERROR: give\n",
      "ERROR: give\n",
      "ERROR: invite\n",
      "ERROR: charge\n",
      "ERROR: charge\n",
      "ERROR: allow\n",
      "ERROR: keep\n",
      "ERROR: motorize\n",
      "ERROR: allow\n",
      "ERROR: leave\n",
      "ERROR: meet\n",
      "ERROR: allot\n",
      "ERROR: give\n",
      "ERROR: charge\n",
      "ERROR: hand\n",
      "ERROR: sign\n",
      "ERROR: give\n",
      "ERROR: specify\n",
      "ERROR: appoint\n",
      "ERROR: appoint\n",
      "ERROR: consist\n",
      "ERROR: consist\n",
      "ERROR: consist\n",
      "ERROR: consist\n",
      "ERROR: teach\n",
      "ERROR: define\n",
      "ERROR: decide\n",
      "ERROR: base\n",
      "ERROR: use\n",
      "ERROR: announce\n",
      "ERROR: reflect\n",
      "ERROR: reserve\n",
      "ERROR: reserve\n",
      "ERROR: reserve\n",
      "ERROR: describe\n",
      "ERROR: give\n",
      "ERROR: apply\n",
      "ERROR: apply\n",
      "ERROR: decide\n",
      "ERROR: receive\n",
      "ERROR: earn\n",
      "ERROR: run\n",
      "ERROR: allow\n",
      "ERROR: register\n",
      "ERROR: allow\n",
      "ERROR: register\n",
      "ERROR: take\n",
      "ERROR: complete\n",
      "ERROR: register\n",
      "ERROR: lead\n",
      "ERROR: do\n",
      "ERROR: do\n",
      "ERROR: inform\n",
      "ERROR: inform\n",
      "ERROR: seek\n",
      "ERROR: include\n",
      "ERROR: permit\n",
      "ERROR: drop\n",
      "ERROR: approve\n",
      "ERROR: cancel\n",
      "ERROR: do\n",
      "ERROR: take\n",
      "ERROR: take\n",
      "ERROR: take\n",
      "ERROR: take\n",
      "ERROR: permit\n",
      "ERROR: avail\n",
      "ERROR: require\n",
      "ERROR: take\n",
      "ERROR: place\n",
      "ERROR: allow\n",
      "ERROR: conduct\n",
      "ERROR: decide\n",
      "ERROR: decide\n",
      "ERROR: decide\n",
      "ERROR: decide\n",
      "ERROR: require\n",
      "ERROR: upload\n",
      "ERROR: mean\n",
      "ERROR: offer\n",
      "ERROR: count\n",
      "ERROR: register\n",
      "ERROR: give\n",
      "ERROR: compute\n",
      "ERROR: announce\n",
      "ERROR: issue\n",
      "ERROR: permit\n",
      "ERROR: permit\n",
      "ERROR: allow\n",
      "ERROR: spend\n",
      "ERROR: allow\n",
      "ERROR: approve\n",
      "ERROR: report\n",
      "ERROR: allow\n",
      "ERROR: use\n",
      "ERROR: compute\n",
      "ERROR: base\n",
      "ERROR: remain\n",
      "ERROR: consider\n",
      "ERROR: consider\n",
      "ERROR: show\n",
      "ERROR: define\n",
      "ERROR: put\n",
      "ERROR: repeat\n",
      "ERROR: place\n",
      "ERROR: place\n",
      "ERROR: apply\n",
      "ERROR: allow\n",
      "ERROR: participate\n",
      "ERROR: permit\n",
      "ERROR: countersign\n",
      "ERROR: countersign\n",
      "ERROR: expect\n",
      "ERROR: attend\n",
      "ERROR: should\n",
      "ERROR: apply\n",
      "ERROR: take\n",
      "ERROR: permit\n",
      "ERROR: permit\n",
      "ERROR: convert\n",
      "ERROR: terminate\n",
      "ERROR: appeal\n",
      "ERROR: terminate\n",
      "ERROR: appeal\n",
      "ERROR: make\n",
      "ERROR: mention\n",
      "ERROR: mention\n",
      "ERROR: pass\n",
      "ERROR: reduce\n",
      "ERROR: reduce\n",
      "ERROR: increase\n",
      "ERROR: increase\n",
      "ERROR: opt\n",
      "ERROR: migrate\n",
      "ERROR: recommend\n",
      "ERROR: earn\n",
      "ERROR: include\n",
      "ERROR: take\n",
      "ERROR: hold\n",
      "ERROR: receive\n",
      "ERROR: sign\n",
      "ERROR: sign\n",
      "ERROR: sign\n",
      "ERROR: sign\n",
      "ERROR: report\n",
      "ERROR: maintain\n",
      "ERROR: tolerate\n",
      "ERROR: result\n",
      "ERROR: result\n",
      "ERROR: result\n",
      "ERROR: expulsion\n",
      "ERROR: formulate\n",
      "ERROR: handle\n",
      "ERROR: formulate\n",
      "ERROR: handle\n",
      "100%|██████████| 246/246 [00:00<00:00, 223236.43it/s]10 10125 20294 30561 40245 51345 171 0.56\n",
      "did, tid, pid, sid, eid, count, percentage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triples = None\n",
    "vertices = None\n",
    "doc_names = [\"UG Regulations\", \"Academic Dishonesty Policy | IIIT-Delhi\", \"Evaluation Policy | IIIT-Delhi\", \"Placement Procedure & Policies | IIIT-Delhi\", \"Green Policy | IIIT-Delhi\", \"Privacy Policy | IIIT-Delhi\", \"Student Conduct Policy | IIIT-Delhi\", \"Refund / Cancellation Policy | IIIT-Delhi\", \"Hostel Policies | IIIT-Delhi\", \"Allocation Policies | IIIT-Delhi\", \"Internships @ IIIT-D | IIIT-Delhi\", \"Disciplinary Action | IIIT-Delhi\", \"Facility Management Services | IIIT-Delhi\", \"B.Tech. Fee Waiver | IIIT-Delhi\"]\n",
    "documents = read_json('../data/files/iiit_website_content.json')[:16] + [{'name': 'UG Regulations', 'file': './../data/files/UG-Regulations', 'format': 'docx', 'link': 'https://www.iiitd.ac.in/sites/default/files/docs/education/2019/2019-July-UG-Regulations.pdf'}]\n",
    "vertices, triples = create_kg(documents)\n",
    "print('did, tid, pid, sid, eid, count, percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json({'vertices': vertices, 'edges': triples}, '../neo4j/iiit_website_graph.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 101 361 725 207 588 done \n",
    "1 87 300 665 202 582 done remove paras with < 5 words\n",
    "1 87 284 648 202 582 done remove paras with < 6 words\n",
    "1 86 264 628 199 579 done remove paras with < 7 words\n",
    "2 91 307 697 241 671 done with 2 docs (+69 sentences, +42 extractions)\n",
    "3 92 318 721 256 723 done with 3 docs (+24 sentences, +15 extractions)\n",
    "3 92 318 721 256 717 done word sense disambiguation (-6 entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}