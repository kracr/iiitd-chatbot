{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform coreference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_text(filename):\n",
    "    raw_text = ''\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            raw_text += line\n",
    "    return raw_text\n",
    "\n",
    "def write_text(text, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for line in text:\n",
    "            file.write(line)\n",
    "            \n",
    "def read_json(filename):\n",
    "    with open(filename) as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def write_json(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually define canonicals and replace in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_predefined_canonicals(triples, canonicals):\n",
    "    for sentence in triples:\n",
    "        for e in range(len(triples[sentence])):\n",
    "            for word in canonicals:\n",
    "                if triples[sentence][e]['subject'] in canonicals[word]:\n",
    "                    triples[sentence][e]['subject'] = word\n",
    "                for obj in range(len(triples[sentence][e]['object'])):\n",
    "                    if triples[sentence][e]['object'][obj] in canonicals[word]:\n",
    "                        triples[sentence][e]['object'][obj] = word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group canonicals and replace with one element of group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSU:\n",
    "    def __init__(self, array):\n",
    "# Code for DSU\n",
    "        self.parent = [i for i in range(len(array))]\n",
    "        self.size = [1 for i in range(len(array))]\n",
    "    def find(self, x):\n",
    "        p = x\n",
    "        while p != self.parent[p]:\n",
    "            p = self.parent[p]\n",
    "        temp = x\n",
    "        while temp != self.parent[temp]:\n",
    "            t = self.parent[temp]\n",
    "            self.parent[temp] = p\n",
    "            temp = t\n",
    "        return p \n",
    "\n",
    "    def combine(self, x, y):\n",
    "        x = self.find(x)\n",
    "        y = self.find(y)\n",
    "        if x == y:\n",
    "            return\n",
    "        if self.size[x] > self.size[y]:\n",
    "            self.parent[y] = x\n",
    "            self.size[x] += self.size[y]\n",
    "        else:\n",
    "            self.parent[x] = y\n",
    "            self.size[y] += self.size[x]\n",
    "\n",
    "# similar if word overlap is greater than 50%\n",
    "def similar(entity1, entity2):\n",
    "    words1 = list(set(entity1.split()))\n",
    "    words2 = list(set(entity2.split()))\n",
    "    common = 0\n",
    "    for word in words1:\n",
    "        common += words2.count(word)\n",
    "    return (2 * common > 0.5 * (len(words1) + len(words2)))\n",
    "\n",
    "\n",
    "def cluster_similar_words(triples):\n",
    "    # All entities\n",
    "    entities = set([])\n",
    "    for sentence in triples:\n",
    "        for extraction in triples[sentence]:\n",
    "            entities.add(extraction['subject'])\n",
    "            for obj in extraction['object']:\n",
    "                entities.add(obj)\n",
    "    entities = list(entities)\n",
    "\n",
    "    dsu = DSU(entities)\n",
    "\n",
    "    # combine similar words\n",
    "    for e1 in range(len(entities)):\n",
    "        for e2 in range(len(entities)):\n",
    "            if similar(entities[e1], entities[e2]):\n",
    "                dsu.combine(e1, e2)\n",
    "\n",
    "    parent_of_word = {}\n",
    "    for e in range(len(entities)):\n",
    "        parent_of_word[entities[e]] = entities[dsu.find(e)]\n",
    "\n",
    "    # replace entity by parent entity\n",
    "    for sentence in triples:\n",
    "        for e in range(len(triples[sentence])):\n",
    "            triples[sentence][e]['subject'] = parent_of_word[triples[sentence][e]['subject']]\n",
    "        for obj in range(len(triples[sentence][e]['object'])):\n",
    "            triples[sentence][e]['object'][obj] = parent_of_word[triples[sentence][e]['object'][obj]]\n",
    "    \n",
    "    groups = {}\n",
    "    for word in parent_of_word:\n",
    "        if parent_of_word[word] not in groups:\n",
    "            groups[parent_of_word[word]] = set([])\n",
    "        groups[parent_of_word[word]].add(word)\n",
    "    for word in groups:\n",
    "        if len(groups[word]) == 1:\n",
    "            continue\n",
    "        print(word)\n",
    "        print(groups[word])\n",
    "    \n",
    "    for e in range(len(entities)):\n",
    "        entities[e] = parent_of_word[entities[e]]\n",
    "    print('original entities:', len(entities))\n",
    "    entities = list(set(entities))\n",
    "    print('reduced entities:', len(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create relation \"similar to\" between canonicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark entity 1 as similar to the entity 2\n",
    "def get_similar_to_edges(triples):\n",
    "    # All entities\n",
    "    entities = set([])\n",
    "    for sentence in triples:\n",
    "        for extraction in triples[sentence]:\n",
    "            entities.add(extraction['subject'])\n",
    "            for obj in extraction['object']:\n",
    "                entities.add(obj)\n",
    "    entities = list(entities)\n",
    "    \n",
    "    similar_to_edges = []\n",
    "    for e1 in range(len(entities)):\n",
    "        for e2 in range(len(entities)):\n",
    "            if e1 == e2:\n",
    "                continue\n",
    "            if similar(entities[e1], entities[e2]):\n",
    "                similar_to_edges.append([entities[e1], 'similar to', entities[e2]])\n",
    "    \n",
    "    return similar_to_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonicalisation using CESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests\n",
    "\n",
    "API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def prepare_cesi_triples(triples, filename):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    cesi_triples = []\n",
    "    id_ = 0\n",
    "    count_linked = 0\n",
    "    count_unlinked = 0\n",
    "    for sentence in triples:\n",
    "        for extraction in triples[sentence]:\n",
    "            for sub in extraction['subject']:\n",
    "                for rel in extraction['relation']:\n",
    "                    for obj in extraction['object']:\n",
    "                        # Get true link of the subject and object from wikidata\n",
    "                        sub_true_link = requests.get(API_ENDPOINT, params = {\n",
    "                                    'action': 'wbsearchentities',\n",
    "                                    'format': 'json',\n",
    "                                    'language': 'en',\n",
    "                                    'search': sub\n",
    "                                }).json()['search']\n",
    "                        if len(sub_true_link) > 0:\n",
    "                            sub_true_link = sub_true_link[0]['id']\n",
    "                            count_linked += 1\n",
    "                        else:\n",
    "                            sub_true_link = None\n",
    "                            count_unlinked += 1\n",
    "                        ob_true_link = requests.get(API_ENDPOINT, params = {\n",
    "                                    'action': 'wbsearchentities',\n",
    "                                    'format': 'json',\n",
    "                                    'language': 'en',\n",
    "                                    'search': obj\n",
    "                                }).json()['search']\n",
    "                        if len(ob_true_link) > 0:\n",
    "                            ob_true_link = ob_true_link[0]['id']\n",
    "                            count_linked += 1\n",
    "                        else:\n",
    "                            ob_true_link = None\n",
    "                            count_unlinked += 1\n",
    "                        print('sub:', sub, sub_true_link, 'obj:', obj, ob_true_link)\n",
    "                        triple = {\n",
    "                            '_id': id_,\n",
    "                            'triple': [\n",
    "                                sub, \n",
    "                                rel, \n",
    "                                obj\n",
    "                            ],\n",
    "                            'triple_norm': [\n",
    "                                ' '.join([wordnet_lemmatizer.lemmatize(word) for word in sub.split()]), \n",
    "                                ' '.join([wordnet_lemmatizer.lemmatize(word) for word in rel.split()]), \n",
    "                                ' '.join([wordnet_lemmatizer.lemmatize(word) for word in obj.split()]), \n",
    "                            ],\n",
    "                            'true_link': {\n",
    "                                'subject': sub_true_link,\n",
    "                                'object': ob_true_link\n",
    "\n",
    "                            },\n",
    "                            'src_sentences': [sentence],\n",
    "                            'entity_linking': {\n",
    "            #                     'subject': entity_links[extraction['subject']],\n",
    "            #                     'object': entity_links[extraction['object']],\n",
    "                            },\n",
    "                            'kbp_info': []\n",
    "                        }\n",
    "                        id_ += 1\n",
    "                        cesi_triples.append(triple)\n",
    "    write_text('\\n'.join([json.dumps(triple) for triple in cesi_triples]), filename)\n",
    "    print('linked:', count_linked, 'unlinked:', count_unlinked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iiitd campus\n",
      "{'vehicles entering iiitd campus', 'campus', 'visitors entering campus', 'iiitd campus'}\n",
      "neha dhiman\n",
      "{'neha dhiman', 'neha'}\n",
      "faculty and staff and students using pathway and faculty visitors\n",
      "{'faculty or staff and students using their vehicles', 'faculty and staff and students using pathway and faculty visitors'}\n",
      "gate\n",
      "{'main gate', 'gate'}\n",
      "such complaints\n",
      "{'such complaints', 'complaints'}\n",
      "visitor\n",
      "{'visitor register', 'entry', 'visitor', 'visitor entry'}\n",
      "food delivery boys vehicle\n",
      "{'food delivery boys', 'delivery of courier', 'food delivery boys vehicle', 'courier boys vehicle', 'courier boys', 'delivery of food'}\n",
      "visitors vehicles\n",
      "{'visitors vehicles', 'vehicles'}\n",
      "vehicle reported by faculty or officers or sc or security for violation of above guidelines\n",
      "{'vehicle reported by faculty or officers or sc or security for violation of above guidelines', 'faculty or officers or sc or security'}\n",
      "facade glass\n",
      "{'facade glass', 'glass'}\n",
      "gate no 3\n",
      "{'no 3', 'gate no 3'}\n",
      "helpdesk\n",
      "{'helpdesk executive', 'helpdesk'}\n",
      "original entities: 102\n",
      "reduced entities: 82\n"
     ]
    }
   ],
   "source": [
    "canonicals = {\n",
    "    'hostel student': ['resident student', 'resident students', 'hostel student', 'hostel students', 'hostel resident', 'hostel residents', 'hosteller', 'hostellers'],\n",
    "    'student': ['student', 'students'],\n",
    "    'instructor': ['instructor', 'professor', 'faculty'],\n",
    "    'campus': ['campus', 'on campus', 'in campus', 'inside campus', 'iiitd', 'in iiitd', 'inside iiitd', 'iiitd campus', 'in iiitd campus', 'inside iiitd campus', 'in college', 'inside college']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "triples = read_json('../data/ollie_triples.json')\n",
    "cluster_similar_words(triples)\n",
    "# write_json(triples, '../data/ollie_canonicalised_2_triples.json')\n",
    "\n",
    "# triples = read_json('../data/ollie_triples.json')\n",
    "# similar_edges = get_similar_to_edges(triples)\n",
    "# print(similar_edges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = read_json('../data/my_extractions.json')\n",
    "\n",
    "words = set([])\n",
    "relations = set([])\n",
    "for sentence in triples:\n",
    "    for triple in triples[sentence]:\n",
    "        for sub in triple['subject']:\n",
    "            words.add(sub)\n",
    "        for obj in triple['object']:\n",
    "            words.add(obj)\n",
    "        for rel in triple['relation']:\n",
    "            relations.append(rel)\n",
    "        for m in triple['modifiers1']:\n",
    "            words.add(m['m_obj'])\n",
    "            relations.add(m['m_rel'])\n",
    "words = '. \\n'.join(words)\n",
    "write_text(words, '../data/my_entities.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub: Visitor entry None obj: 8 am Q41618176\n",
      "sub: Visitors Q1076940 obj: access Q80689\n",
      "sub: All visitors None obj: CCTV surveillance Q101069887\n",
      "sub: number plate Q22706 obj: camera Q15328\n",
      "sub: Gate No 3 None obj: open Q2735683\n",
      "sub: The entry Q5452326 obj: Faculty Q180958\n",
      "sub: The entry Q5452326 obj: staff Q703534\n",
      "sub: The entry Q5452326 obj: Students Q48282\n",
      "sub: The vehicles Q100476087 obj: Gate No 1 None\n",
      "sub: The Faculty Q373267 obj: IIITD Campus None\n",
      "sub: their visitors None obj: Gate No 3 None\n",
      "sub: The vehicles Q100476087 obj: The vehicles Q100476087\n",
      "sub: IIITD Campus None obj: No Horn Zone None\n",
      "sub: IIITD Campus None obj: maximum speed Q1077350\n",
      "sub: hr Q224 obj: permissible Q4376583\n",
      "sub: All Faculty None obj: sticker Q2872553\n",
      "sub: staff Q703534 obj: sticker Q2872553\n",
      "sub: students Q48282 obj: sticker Q2872553\n",
      "sub: All visitors vehicles None obj: necessary registration None\n",
      "sub: Any vehicle None obj: Faculty Q180958\n",
      "sub: Any vehicle None obj: Officers Q4340308\n",
      "sub: Any vehicle None obj: SC Q1456\n",
      "sub: Security Q2526135 obj: Rs 200 Q2124609\n",
      "sub: The Helpdesk None obj: Email Q9158\n",
      "sub: The Helpdesk Executive None obj: complaint Q836925\n",
      "sub: Complaints which None obj: assistance Q1179505\n",
      "sub: OEM Q267558 obj: FM Q702\n",
      "sub: OEM Q267558 obj: GM(Ops None\n",
      "sub: complaint Q836925 obj: subject Q830077\n",
      "sub: The SLA Q1399299 obj: 2 hrs None\n",
      "sub: The SLA Q1399299 obj: 24 Hrs Q27827476\n",
      "sub: AC Q81292 obj: https://iiitd.ac.in/sites/default/files/docs/life/ac-schedule-for-hostels.pdf None\n",
      "sub: Heating Schedule None obj: https://iiitd.ac.in/sites/default/files/docs/life/ac-schedule-for-hostels.pdf None\n",
      "sub: Floors Q99968674 obj: hostel Q654772\n",
      "sub: Toilets Q7857 obj: three times day None\n",
      "sub: Stairs Q12511 obj: hostel Q654772\n",
      "sub: Common rooms None obj: hostel Q654772\n",
      "sub: Lifts Q99372 obj: hostel Q654772\n",
      "sub: Floors Q99968674 obj: campus Q209465\n",
      "sub: Toilets Q7857 obj: three times day None\n",
      "sub: Glass Q11469 obj: campus Q209465\n",
      "sub: Stairs Q12511 obj: campus Q209465\n",
      "sub: Lifts Q99372 obj: campus Q209465\n",
      "sub: garbage Q45701 obj: campus Q209465\n",
      "sub: Roads Q54229562 obj: campus Q209465\n",
      "sub: Common areas Q35160876 obj: campus Q209465\n",
      "sub: Labs Q933331 obj: campus Q209465\n",
      "sub: classrooms Q58885529 obj: campus Q209465\n",
      "sub: Fans Q193432 obj: month Q5151\n",
      "sub: lights Q9128 obj: month Q5151\n",
      "sub: Facade glass None obj: month Q5151\n",
      "sub: Fire drill Q3243704 obj: every semester None\n",
      "sub: The schedule Q48914414 obj: advance Q379156\n",
      "sub: Dr. RK Katharya B.Sc None obj: available Q64678853\n",
      "sub: MBBS Q13948235 obj: available Q64678853\n",
      "sub: FCPG Q4049685 obj: available Q64678853\n",
      "sub: A qualified Nurse Ms. Neha Dhiman None obj: mobile No 9650907449 None\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/my_cesi/my_cesi_triples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f7682c4ac665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run java -Xmx16g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,entitylink -file ../data/my_entities.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# entity_links = read_json('../data/entity_links.json')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprepare_cesi_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../data/my_cesi/my_cesi_triples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# # replace_predefined_canonicals(triples, canonicals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# write_json(triples, '../data/ollie_canonicalised_1_triples.json')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-87de8abea395>\u001b[0m in \u001b[0;36mprepare_cesi_triples\u001b[0;34m(triples, filename)\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mid_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0mcesi_triples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mwrite_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcesi_triples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'linked:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_linked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unlinked:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_unlinked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c4aed4373bf7>\u001b[0m in \u001b[0;36mwrite_text\u001b[0;34m(text, filename)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/my_cesi/my_cesi_triples'"
     ]
    }
   ],
   "source": [
    "# run java -Xmx16g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,entitylink -file ../data/my_entities.txt\n",
    "# entity_links = read_json('../data/entity_links.json')\n",
    "prepare_cesi_triples(triples, '../data/my_cesi/my_cesi_triples')\n",
    "# # replace_predefined_canonicals(triples, canonicals)\n",
    "# write_json(triples, '../data/ollie_canonicalised_1_triples.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
